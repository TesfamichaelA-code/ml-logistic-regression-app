# Logistic Regression ML Pipeline

A complete end-to-end machine learning pipeline using **Logistic Regression** to predict Titanic passenger survival.

## Project Structure

```
ml-logistic-regression-app/
├── README.md                              # Project documentation
├── backend/
│   └── main.py                           # FastAPI backend (for deployment)
├── model/
│   └── logistic_model.joblib             # Trained model (generated by notebook)
└── notebook/
    └── logistic_regression_pipeline.ipynb # Complete ML pipeline notebook
```

## Features

- **Data Loading**: Titanic dataset from public URL
- **Exploratory Data Analysis**: Comprehensive visualizations
- **Data Cleaning**: Missing value handling, feature selection
- **Feature Engineering**: FamilySize, IsAlone, AgeGroup, FarePerPerson
- **sklearn Pipeline**: Preprocessing + Model in a single pipeline
- **Model Training**: Logistic Regression with balanced class weights
- **Evaluation**: Accuracy, Precision, Recall, F1, Confusion Matrix, ROC-AUC
- **Model Export**: Saved as `logistic_model.joblib`

## Quick Start

### Prerequisites

```bash
pip install pandas numpy matplotlib seaborn scikit-learn joblib
```

### Run the Notebook

1. Open `notebook/logistic_regression_pipeline.ipynb`
2. Run all cells top-to-bottom
3. The trained model will be saved to `model/logistic_model.joblib`

### Use the Trained Model

```python
import joblib
import pandas as pd

# Load the model
model = joblib.load('model/logistic_model.joblib')

# Create new passenger data
new_passenger = pd.DataFrame({
    'Pclass': [1],
    'Sex': ['female'],
    'Age': [25],
    'SibSp': [1],
    'Parch': [0],
    'Fare': [100.0],
    'Embarked': ['S'],
    'FamilySize': [2],
    'IsAlone': [0],
    'FarePerPerson': [50.0],
    'AgeGroup': ['YoungAdult']
})

# Make prediction
prediction = model.predict(new_passenger)
probability = model.predict_proba(new_passenger)

print(f"Survival Prediction: {'Survived' if prediction[0] == 1 else 'Did Not Survive'}")
print(f"Probability of Survival: {probability[0][1]*100:.2f}%")
```

## Model Performance

| Metric    | Training | Test |
| --------- | -------- | ---- |
| Accuracy  | ~82%     | ~80% |
| Precision | ~79%     | ~77% |
| Recall    | ~75%     | ~73% |
| F1-Score  | ~77%     | ~75% |
| ROC-AUC   | ~87%     | ~85% |

## Pipeline Architecture

```
Input Data
    │
    ▼
┌─────────────────────────────────────────────┐
│              ColumnTransformer              │
│  ┌─────────────────┬─────────────────────┐  │
│  │   Numerical     │    Categorical      │  │
│  │   Pipeline      │    Pipeline         │  │
│  │  ┌───────────┐  │  ┌───────────────┐  │  │
│  │  │ Imputer   │  │  │ Imputer       │  │  │
│  │  │ (median)  │  │  │ (most_freq)   │  │  │
│  │  └─────┬─────┘  │  └───────┬───────┘  │  │
│  │        ▼        │          ▼          │  │
│  │  ┌───────────┐  │  ┌───────────────┐  │  │
│  │  │ Standard  │  │  │ OneHotEncoder │  │  │
│  │  │ Scaler    │  │  │               │  │  │
│  │  └───────────┘  │  └───────────────┘  │  │
│  └─────────────────┴─────────────────────┘  │
└─────────────────────────────────────────────┘
                     │
                     ▼
         ┌─────────────────────┐
         │ Logistic Regression │
         │  (balanced weights) │
         └─────────────────────┘
                     │
                     ▼
              Predictions
```

## License

MIT License
